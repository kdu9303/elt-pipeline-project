version: '3.8'
services:
  kafdrop:
    image: obsidiandynamics/kafdrop
    container_name: kafdrop
    restart: always
    ports:
      - "9000:9000"
    environment:
      KAFKA_BROKER_CONNECT: |-
        43.201.13.181:9092,
        43.200.251.62:9092,
        52.78.78.140:9092
  schema-registry:
    image: confluentinc/cp-schema-registry:6.1.1
    container_name: schema-registry
    restart: always
    ports:
      - 8081:8081
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "PLAINTEXT://43.201.13.181:9092,PLAINTEXT://43.200.251.62:9092,PLAINTEXT://52.78.78.140:9092"
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
  kafka-connect:
    image: confluentinc/cp-kafka-connect-base:6.1.0
    user: root
    container_name: kafka-connect
    restart: always
    depends_on:
      - schema-registry
    ports:
      - 8083:8083
      - 7075:7075
    environment:
      CONNECT_BOOTSTRAP_SERVERS: |-
        "43.201.13.181:9092,
        43.200.251.62:9092,
        52.78.78.140:9092"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: kafka-connect
      CONNECT_CONFIG_STORAGE_TOPIC: _kafka-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _kafka-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _kafka-connect-status
      # JMX export
      KAFKA_OPTS: -javaagent:/usr/share/jmx_exporter/jmx_prometheus_javaagent-0.17.2.jar=7075:/usr/share/jmx_exporter/kafka-connect.yml -Xmx256M -Xms256M
      # External secrets configs.
      CONNECT_CONFIG_PROVIDERS: 'file'
      CONNECT_CONFIG_PROVIDERS_FILE_CLASS: 'org.apache.kafka.common.config.provider.FileConfigProvider'
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      # AWS_SESSION_TOKEN: ${AWS_SESSION_TOKEN}
      AWS_DEFAULT_REGION: "ap-northeast-2"
      CONNECT_KAFKA_HEAP_OPTS: "-Xms256M -Xmx2G"
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_PLUGIN_PATH: '/usr/share/java,/usr/share/confluent-hub-components/,/connectors/'
      # If you want to use the Confluent Hub installer to d/l component, but make them available
      # when running this offline, spin up the stack once and then run :
      #   docker cp kafka-connect:/usr/share/confluent-hub-components ./connectors
      #   mv ./connectors/confluent-hub-components/* ./connectors
      #   rm -rf ./connectors/confluent-hub-components
    volumes:
      - ./connectors:/connectors
      - ./config:/config
      - ./jmx_exporter:/usr/share/jmx_exporter
      # - $HOME/.aws/:/root/.aws:ro
    # connector 플러그인, RDB driver 설치 필요
    command:
      - bash
      - -c
      - |
        echo "Installing connector plugins"
        confluent-hub install --no-prompt mdrogalis/voluble:0.3.1
        confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:5.5.3
        confluent-hub install --no-prompt confluentinc/kafka-connect-s3:10.1.1
        # confluent-hub install --no-prompt mongodb/kafka-connect-mongodb:1.7.0
        # Sticking to 5.5.3 at the moment because of issue with 10.0.1 https://rmoff.net/2021/03/11/kafka-connect-sqlsyntaxerrorexception-blob/text-column-used-in-key-specification-without-a-key-length/
        #
        echo "Downloading JDBC driver"
        cd /usr/share/confluent-hub-components/confluentinc-kafka-connect-jdbc
        # plugins 폴더에 jdbc driver를 다운받는다(DB버전 확인 필수)
        wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.27/mysql-connector-java-8.0.27.jar
        wget https://repo1.maven.org/maven2/com/oracle/database/jdbc/ojdbc8/21.5.0.0/ojdbc8-21.5.0.0.jar
        # curl https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.29.tar.gz | tar xz
        #
        # JMX exporter setup
        echo "setting up JMX exporter for kafka-connect"
        cd /usr/share/jmx_exporter
        wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.17.2/jmx_prometheus_javaagent-0.17.2.jar
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run &
        #
        sleep infinity
  kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    container_name: kafka-exporter
    restart: always
    ports:
      - 9308:9308
    command:
      - --kafka.server=43.201.13.181:9092
      - --kafka.server=43.200.251.62:9092
      - --kafka.server=52.78.78.140:9092
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: always
    volumes:
      - ./prometheus/:/etc/prometheus/
    ports:
      - 9090:9090
    command:
      - '--web.enable-lifecycle'
      - '--config.file=/etc/prometheus/prometheus.yml'

  # unhealty 컨테이너를 재시작한다
  autoheal:
    image: willfarrell/autoheal:latest
    tty: true
    container_name: autoheal
    restart: always
    environment:
      AUTOHEAL_CONTAINER_LABEL: all
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
