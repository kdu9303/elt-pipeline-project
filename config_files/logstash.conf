input {

    kafka {
        bootstrap_servers =>  "43.201.13.181:9092, 43.200.251.62:9092, 52.78.78.140:9092"
        group_id => "logstash"
        topics => "spark-application-log"
        codec => plain { charset => "UTF-8"}
        consumer_threads => 2
    }
}
filter {
    mutate {
        gsub => [
            "message", "\u0000", "",
            "message", "\n", "",
            "message", "[\\]", ""
        ]
    }
    grok {
        match => {
            "message" => [" %{LOGLEVEL:log_level}"]
        }
    }
    grok {
        match => {
            "message" => ["%{USERNAME:host_name}"]
        }
    }
    grok {
        match => {
            "message" => ["%{DATESTAMP:spark_datestamp}"]
        }
    }
}

output {

    s3 {
        region => "ap-northeast-2"
        bucket => "etl-project-bucket-20220817"
        prefix => "spark_application_log/%{+YYYY}/%{+MM}/%{+dd}"
        codec => "json_lines"
  }
}
